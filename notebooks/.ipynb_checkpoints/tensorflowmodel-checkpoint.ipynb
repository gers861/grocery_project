{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61b323a-a379-4295-8a2e-664ffdcb2faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - binary_accuracy: 0.5042 - loss: 0.6933 - val_binary_accuracy: 0.5159 - val_loss: 0.6929\n",
      "Epoch 2/10\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - binary_accuracy: 0.7921 - loss: 0.6853 - val_binary_accuracy: 0.6384 - val_loss: 0.6862\n",
      "Epoch 3/10\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - binary_accuracy: 0.8831 - loss: 0.6596 - val_binary_accuracy: 0.7311 - val_loss: 0.6484\n",
      "Epoch 4/10\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - binary_accuracy: 0.8527 - loss: 0.5880 - val_binary_accuracy: 0.7467 - val_loss: 0.5864\n",
      "Epoch 5/10\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - binary_accuracy: 0.8348 - loss: 0.5021 - val_binary_accuracy: 0.7496 - val_loss: 0.5487\n",
      "Epoch 6/10\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - binary_accuracy: 0.8424 - loss: 0.4397 - val_binary_accuracy: 0.7503 - val_loss: 0.5347\n",
      "Epoch 7/10\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - binary_accuracy: 0.8553 - loss: 0.4000 - val_binary_accuracy: 0.7494 - val_loss: 0.5313\n",
      "Epoch 8/10\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - binary_accuracy: 0.8751 - loss: 0.3601 - val_binary_accuracy: 0.7478 - val_loss: 0.5326\n",
      "Epoch 9/10\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - binary_accuracy: 0.8941 - loss: 0.3294 - val_binary_accuracy: 0.7449 - val_loss: 0.5368\n",
      "Epoch 10/10\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - binary_accuracy: 0.9084 - loss: 0.3021 - val_binary_accuracy: 0.7419 - val_loss: 0.5427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1da1008c400>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('../data/Groceries_cleaned_dataset2.csv')\n",
    "\n",
    "# Encode user and item IDs as integers for embeddings\n",
    "user_ids = df['Member_number'].unique()\n",
    "item_ids = df['itemDescription'].unique()\n",
    "\n",
    "user2idx = {u: i for i, u in enumerate(user_ids)}\n",
    "item2idx = {i: j for j, i in enumerate(item_ids)}\n",
    "\n",
    "df['user_idx'] = df['Member_number'].map(user2idx)\n",
    "df['item_idx'] = df['itemDescription'].map(item2idx)\n",
    "\n",
    "# Create positive interactions\n",
    "positive_interactions = df[['user_idx', 'item_idx']].drop_duplicates()\n",
    "\n",
    "# Negative sampling function\n",
    "def generate_negative_samples(pos_df, num_users, num_items, num_neg=1):\n",
    "    negatives = []\n",
    "    user_item_set = set(zip(pos_df['user_idx'], pos_df['item_idx']))\n",
    "    for (u, i) in user_item_set:\n",
    "        for _ in range(num_neg):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in user_item_set:\n",
    "                j = np.random.randint(num_items)\n",
    "            negatives.append([u, j])\n",
    "    neg_df = pd.DataFrame(negatives, columns=['user_idx', 'item_idx'])\n",
    "    neg_df['label'] = 0\n",
    "    return neg_df\n",
    "\n",
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "\n",
    "# Label positive interactions as 1\n",
    "positive_interactions['label'] = 1\n",
    "\n",
    "# Generate negative samples (1 negative per positive)\n",
    "negative_interactions = generate_negative_samples(positive_interactions, num_users, num_items, num_neg=1)\n",
    "\n",
    "# Combine positive and negative samples\n",
    "data = pd.concat([positive_interactions, negative_interactions])\n",
    "\n",
    "# Shuffle data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare TensorFlow datasets\n",
    "def df_to_dataset(df):\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            \"user_idx\": df['user_idx'].values,\n",
    "            \"item_idx\": df['item_idx'].values\n",
    "        },\n",
    "        df['label'].values\n",
    "    )).batch(256).shuffle(10000)\n",
    "\n",
    "train_ds = df_to_dataset(train)\n",
    "test_ds = df_to_dataset(test)\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 32\n",
    "\n",
    "# Build model\n",
    "class MFModel(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_dim):\n",
    "        super(MFModel, self).__init__()\n",
    "        self.user_embedding = tf.keras.layers.Embedding(num_users, embedding_dim,\n",
    "                                                        embeddings_initializer='he_normal',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.l2(1e-6))\n",
    "        self.item_embedding = tf.keras.layers.Embedding(num_items, embedding_dim,\n",
    "                                                        embeddings_initializer='he_normal',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.l2(1e-6))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs['user_idx'])\n",
    "        item_vector = self.item_embedding(inputs['item_idx'])\n",
    "        dot = tf.reduce_sum(user_vector * item_vector, axis=1)\n",
    "        # Use sigmoid to get probability\n",
    "        return tf.nn.sigmoid(dot)\n",
    "\n",
    "model = MFModel(num_users, num_items, embedding_dim)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(train_ds, epochs=10, validation_data=test_ds)\n",
    "\n",
    "# After training, you can predict for user-item pairs like:\n",
    "# preds = model({\"user_idx\": np.array([user_index]), \"item_idx\": np.array([item_index])})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc5000-0ee6-4272-b755-a7c175641a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
